{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a16313",
   "metadata": {},
   "source": [
    "## Training CNN on Multiple Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c3bdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:GPU:0', '/job:worker/replica:0/task:1/device:CPU:0', '/job:worker/replica:0/task:1/device:GPU:0', '/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:GPU:0', '/job:worker/replica:0/task:2/device:CPU:0', '/job:worker/replica:0/task:2/device:GPU:0']\n",
      "INFO:tensorflow:Check health not enabled.\n",
      "INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['192.168.10.1:8888'], 'worker': ['192.168.10.2:8888', '192.168.10.3:8888', '192.168.10.4:8888']}, task_type = 'chief', task_id = 0, num_workers = 4, local_devices = ('/job:chief/task:0/device:GPU:0',), communication = CommunicationImplementation.NCCL\n"
     ]
    }
   ],
   "source": [
    "# Importing Librairies\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, InputLayer, Reshape, Rescaling\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "#from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Force using CPU by making GPU invisible\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# TF_CONFIG envrionment variable declaration\n",
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    'cluster': {\n",
    "        \"chief\": [\"192.168.10.1:8888\"],\n",
    "        'worker': [\"192.168.10.2:8888\", \"192.168.10.3:8888\", \"192.168.10.4:8888\"]\n",
    "    },\n",
    "    'task': {'type': 'chief', 'index': 0}\n",
    "    #'task': {'type': 'worker', 'index': 0}\n",
    "})\n",
    "\n",
    "\n",
    "# NCCL option\n",
    "options = tf.distribute.experimental.CommunicationOptions(\n",
    "    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "\n",
    "# Declaring Strategy before inserting Data and creating Model\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874c235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing & Preprocessing CIFAR-10 Dataset from Directory\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "data_path = './CIFAR-10/'\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    # Loading Training Dataset\n",
    "    x_train = np.empty((50000, 32, 32, 3), dtype='uint8')\n",
    "    y_train = np.empty((50000, ), dtype='uint8')\n",
    "    i = 1    \n",
    "    while i <= 5:\n",
    "        filename = 'data_batch_' + str(i)\n",
    "        fullpath = os.path.join(data_path, filename)\n",
    "        data_batch = unpickle(fullpath)\n",
    "        \n",
    "        x = data_batch[b'data']\n",
    "        x_train[(i - 1) * 10000:i * 10000, :, :, :] = x.reshape(len(x), 3, img_height, img_width).transpose(0, 2, 3, 1)\n",
    "        y_train[(i - 1) * 10000:i * 10000] = data_batch[b'labels']\n",
    "        i = i + 1\n",
    "    \n",
    "    # Loading Test/Validation Dataset\n",
    "    fullpath = os.path.join(data_path, 'test_batch')\n",
    "    test_batch = unpickle(fullpath)\n",
    "    x_test = test_batch[b'data']\n",
    "    x_test = x_test.reshape(len(x_test), 3, img_height, img_width).transpose(0, 2, 3, 1)\n",
    "    y_test = test_batch[b'labels']\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    # Normalizing Images\n",
    "    x_train, x_test = normalize_images(x_train, x_test)\n",
    "    # Display Dataset Size\n",
    "    print('Training Images : {}'.format(len(x_train)))\n",
    "    print('Test Images : {}'.format(len(x_test)))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def normalize_images(train, test):\n",
    "    # convert 0-255 integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # scaling values from 0-255 to 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    "\n",
    "\n",
    "# Function : Create and Compile Model\n",
    "def create_and_compile_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=[accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_replica_batch_size = 1024\n",
    "global_batch_size = per_replica_batch_size * strategy.num_replicas_in_sync\n",
    "\n",
    "# load CIFAR-10 Dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_dataset()\n",
    "\n",
    "# Data Augmentation Generator\n",
    "datagen = ImageDataGenerator(\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True)\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Data iterator generating training batches\n",
    "iter_train = datagen.flow(x_train, y_train, batch_size=global_batch_size)\n",
    "# Calculating Steps par Epoch\n",
    "steps = int(len(x_train) / global_batch_size)\n",
    "\n",
    "with strategy.scope():\n",
    "    # Creating and Compiling Model inside the Strategy Scope\n",
    "    model = create_and_compile_model()    \n",
    "    \n",
    "# Training the model without callbacks\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=global_batch_size), \n",
    "                    validation_data=(x_test, y_test), steps_per_epoch=steps, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on test dataset\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"test loss : {:.4f}\".format(score[0]))\n",
    "print(\"test accuracy : {:.4f}\".format(score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
